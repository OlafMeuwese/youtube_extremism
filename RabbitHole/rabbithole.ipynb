{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YouTube Reccomendation Cleaner ##\n",
    "\n",
    "This script processes, cleans and enrich the YouTube recommendation data gathered trhough an experiment by De Volkskrant and De Correspondent. Participants (n=78) were asked to run a script [ad link] on their computer. This script searched for ten key words on YouTube and clicked on ten recommendations and repeated the last step 3 times (so depth is 4). We also asked the participants to provide some personal data. Identifying data has been hashed, to protect the privacy of the participants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: google-api-python-client in /home/dim/Environments/youtube/lib/python3.6/site-packages (1.7.4)\n",
      "Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /home/dim/Environments/youtube/lib/python3.6/site-packages (from google-api-python-client) (1.5.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /home/dim/Environments/youtube/lib/python3.6/site-packages (from google-api-python-client) (0.0.3)\n",
      "Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /home/dim/Environments/youtube/lib/python3.6/site-packages (from google-api-python-client) (3.0.0)\n",
      "Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /home/dim/Environments/youtube/lib/python3.6/site-packages (from google-api-python-client) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /home/dim/Environments/youtube/lib/python3.6/site-packages (from google-api-python-client) (0.11.3)\n",
      "Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /home/dim/Environments/youtube/lib/python3.6/site-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /home/dim/Environments/youtube/lib/python3.6/site-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /home/dim/Environments/youtube/lib/python3.6/site-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /home/dim/Environments/youtube/lib/python3.6/site-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install --upgrade google-api-python-client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import csv\n",
    "import hashlib\n",
    "from config import *\n",
    "\n",
    "#and import some Google API libraries for later on\n",
    "from apiclient.discovery import build\n",
    "from apiclient.errors import HttpError\n",
    "#from oauth2client.tools import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load json documents from the experiment and append them to dataframe\n",
    "\n",
    "df = pd.DataFrame()\n",
    "path = config.path_raw_data\n",
    "path_temp = config.path_temp_data\n",
    "\n",
    "for file in glob.glob(path + '/**/*.json', recursive=True):\n",
    "    try:\n",
    "        name = os.path.basename(file)\n",
    "        name = re.sub('youtube(-onderzoek)?-(mac|win)-\\s?', '', name)\n",
    "        name = re.sub('-\\d{4}-\\d{2}-\\d{2}\\.json', '', name)\n",
    "        dirname = os.path.dirname(file)\n",
    "        dirname = dirname.replace('/Users/mindyourownbusiness/Desktop/resultaten_yt/', '')\n",
    "        df_temp = json.load(open(file))\n",
    "        for item in df_temp:\n",
    "            item['search_term'] = name\n",
    "            item['subject'] = dirname\n",
    "        df = df.append(df_temp)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hash email adresses in dataframe\n",
    "\n",
    "df['user_hash'] = [hashlib.md5(val).hexdigest() for val in df['subject'].str.encode('utf-8')]\n",
    "df.drop('subject', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write dataframe to file\n",
    "\n",
    "df.to_csv(path_temp + 'recommendations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract titles from YouTube ##\n",
    "\n",
    "Most recommendations are in a list and only the video_id is given. It is therefore necessary to extract the lists to rows and use the video_id to query the YouTube API for extra iformation, like title, channel, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vids = pd.read_csv(path_temp + 'recommendations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extract the video ids from the list in the recommendations column\n",
    "\n",
    "def splitDataFrameList(df,target_column,separator):\n",
    "    ''' df = dataframe to split,\n",
    "    target_column = the column containing the values to split\n",
    "    separator = the symbol used to perform the split\n",
    "    returns: a dataframe with each entry for the target column separated, with each element moved into a new row. \n",
    "    The values in the other columns are duplicated across the newly divided rows.\n",
    "    '''\n",
    "    row_accumulator = []\n",
    "\n",
    "    def splitListToRows(row, separator):\n",
    "        split_row = row[target_column].split(separator)\n",
    "        for s in split_row:\n",
    "            new_row = row.to_dict()\n",
    "            new_row[target_column] = s\n",
    "            row_accumulator.append(new_row)\n",
    "\n",
    "    df.apply(splitListToRows, axis=1, args = (separator, ))\n",
    "    new_df = pd.DataFrame(row_accumulator)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vids_new = splitDataFrameList(vids, 'recommendations', ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# and clean it up a bit\n",
    "\n",
    "vids_new.recommendations = vids_new.recommendations.astype(str) #convert to strings\n",
    "vids_new.recommendations = vids_new.recommendations.str.replace(\"'|\\[|\\]|\\s\", '') #remove some clutter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write extracted and clean version to file\n",
    "\n",
    "vids_new.to_csv(path_temp + 'recommendations_unpacked.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the file and find the unique video ids to query YouTube\n",
    "\n",
    "vids_new = pd.read_csv(path_temp + 'recommendations_unpacked.csv')\n",
    "lookup = vids_new.recommendations.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load credentials from config file\n",
    "\n",
    "DEVELOPER_KEY = config.DEVELOPER_KEY\n",
    "YOUTUBE_API_SERVICE_NAME = config.YOUTUBE_API_SERVICE_NAME\n",
    "YOUTUBE_API_VERSION = config.YOUTUBE_API_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load search function\n",
    "\n",
    "def youtube_search(video_id):\n",
    "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
    "                    developerKey=DEVELOPER_KEY)\n",
    "    search_response = youtube.videos().list(\n",
    "    id = video_id,\n",
    "    part=\"snippet\",\n",
    "    ).execute()\n",
    "    return search_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#query the YouTube API. Due to the large amount of videos I think it's better to break the list up in chunks. \n",
    "\n",
    "#lookup = lookup[230179:] you can uncomment this when the session breaks, e.g. due to ssl problems (which I had). Just continue from where you left off.\n",
    "total_count = len(lookup)\n",
    "\n",
    "chunks = total_count // 200 + 1\n",
    "\n",
    "for i in range(chunks):\n",
    "    batch = lookup[i*200:(i+1)*200]\n",
    "    for video_id in batch:\n",
    "        video_json = youtube_search(video_id)\n",
    "        try:\n",
    "            for video in video_json['items']:\n",
    "                Id = video['id']\n",
    "                publishedAt = video['snippet']['publishedAt']\n",
    "                title = video['snippet']['title']\n",
    "                channelId = video['snippet']['channelId']\n",
    "                channelTitle = video['snippet']['channelTitle']\n",
    "                #description = video['snippet']['description']\n",
    "                with open(path_temp + \"results.csv\", \"a\") as csvFile:\n",
    "                    fieldnames = ['id', 'publishedAt', 'title', 'channelId', 'channelTitle']\n",
    "                    writer = csv.DictWriter(csvFile, fieldnames)\n",
    "                    writer.writerow({'id': Id, 'publishedAt': publishedAt, 'title': title, 'channelId': channelId, 'channelTitle': channelTitle})\n",
    "                del video\n",
    "        except:\n",
    "            continue\n",
    "    del batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compare columns of lookup and results to see if we haven't missed anything.\n",
    "\n",
    "results = pd.read_csv(path_temp + 'results.csv', header=None, names=['id', 'date', 'title', 'channel_id', 'channel_name'])\n",
    "lookup = vids_new.recommendations.unique()\n",
    "lookup = pd.DataFrame(lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lookup.columns = ['id']\n",
    "lookup.sort_values(by=['id'], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results.sort_values(by=['id'], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "todo = pd.Index.symmetric_difference(pd.Index(results.id), pd.Index(lookup.id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vids = []\n",
    "\n",
    "for vid in todo:\n",
    "    vids.append(vid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring it all together\n",
    "\n",
    "So now we have three csv files:\n",
    "1. recommendations_unpacked.csv - this is the raw json data from the experiment, flattened into a dataframe.\n",
    "2. antwoorden_yt_experiment.csv - this are the answers from the subjects that can be used as filters (edge attributes if we use a graph).\n",
    "3. results.csv - this contains the extracted lists of the first df['recommendations'], appended with video ids and titles, channel ids and titles and descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv(path_temp_data + '/antwoorden_yt_experiment.csv', sep=';', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "users['user_hash'] = [hashlib.md5(val).hexdigest() for val in users['mail'].str.encode('utf-8')]\n",
    "users.drop('mail', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['video_id', 'upload_date', 'video_title', 'channel_id', 'channel_title']\n",
    "results = pd.read_csv(path_temp + 'results.csv', names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results.sort_values(by='video_id', axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recommendations = pd.read_csv('recommendations_unpacked.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recommendations.rename(index=str, columns={\"recommendations\": \"video_id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "semi_final = pd.merge(recommendations, users, on='user_hash', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yt = pd.merge(semi_final, results, on='video_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#and it still needs some cleaning\n",
    "\n",
    "yt.drop(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'key', 'mult', 'nb_recommendations', 'Timestamp', 'yt_gebruik', 'sociale_media', 'deel_computer'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in ['depth', 'search_term', 'user_hash', 'woonplaats', 'geslacht', 'opleiding', 'politiek']:\n",
    "    yt[col] = yt[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yt.columns = ['from_channel_title', \n",
    "              'depth', \n",
    "              'dislikes', \n",
    "              'likes', \n",
    "              'from_video_id', \n",
    "              'search_term', \n",
    "              'from_video_title', \n",
    "              'user_hash',\n",
    "             'views',\n",
    "             'woonplaats',\n",
    "             'leeftijd',\n",
    "             'geslacht',\n",
    "             'opleiding',\n",
    "             'politiek',\n",
    "             'upload_date',\n",
    "             'to_video_title',\n",
    "             'to_channel_id', \n",
    "             'to_channel_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yt = yt[['from_video_id',\n",
    "         'from_video_title',\n",
    "         'from_channel_title', \n",
    "         'depth',\n",
    "         'likes',\n",
    "         'dislikes',\n",
    "         'views',\n",
    "         'to_video_title',\n",
    "         'upload_date',\n",
    "         'to_channel_id', \n",
    "         'to_channel_title',\n",
    "         'search_term',\n",
    "         'user_hash',\n",
    "         'woonplaats',\n",
    "         'leeftijd',\n",
    "         'geslacht',\n",
    "         'opleiding',\n",
    "         'politiek']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yt = yt.dropna(subset=['from_video_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "yt = yt.dropna(subset=['to_video_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yt = yt.dropna(subset=['user_hash'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write result to csv\n",
    "\n",
    "yt.to_csv(path_temp + 'yt_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. You can find the Notebook for the analysis of the data here [ad link]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
