{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "#import youtubecollector as ytc\n",
    "import sys\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append videos in one videofile and drop the duplicates in the separate file\n",
    "\n",
    "#output_file = 'output/left/videos_left.csv'\n",
    "#input_files = 'output/left/raw_data/videos_left_*.csv'\n",
    "\n",
    "output_file = 'output/right/videos_right.csv'\n",
    "input_files = 'output/right/raw_data/videos_right_*.csv'\n",
    "\n",
    "column_names = ['video_id',\n",
    "              'video_published',\n",
    "              'channel_id',\n",
    "              'video_title',\n",
    "              'video_description',\n",
    "              'video_channel_title',\n",
    "              'video_tags',\n",
    "              'video_category_id',\n",
    "              'video_default_language',\n",
    "              'video_duration',\n",
    "              'video_view_count',\n",
    "              'video_comment_count',\n",
    "              'video_likes_count',\n",
    "              'video_dislikes_count',\n",
    "              'video_topic_ids',\n",
    "              'video_topic_categories'\n",
    "               ]\n",
    "\n",
    "\n",
    "for filename in glob.glob(input_files):\n",
    "    file = pd.read_csv(filename)\n",
    "    df = file.drop_duplicates()\n",
    "    if not os.path.isfile(output_file):\n",
    "           df.to_csv(output_file, \n",
    "                     mode='a', \n",
    "                     header=column_names,\n",
    "                     sep='¶',\n",
    "                     quotechar='þ',\n",
    "                    )\n",
    "    else: \n",
    "           df.to_csv(output_file, \n",
    "                     mode='a', \n",
    "                     header=False,\n",
    "                     sep='¶',\n",
    "                     quotechar='þ'\n",
    "                    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#drop the duplicates in the appended file\n",
    "input_file = 'output/right/videos_right.csv'\n",
    "column_names = ['video_id',\n",
    "              'video_published',\n",
    "              'channel_id',\n",
    "              'video_title',\n",
    "              'video_description',\n",
    "              'video_channel_title',\n",
    "              'video_tags',\n",
    "              'video_category_id',\n",
    "              'video_default_language',\n",
    "              'video_duration',\n",
    "              'video_view_count',\n",
    "              'video_comment_count',\n",
    "              'video_likes_count',\n",
    "              'video_dislikes_count',\n",
    "              'video_topic_ids',\n",
    "              'video_topic_categories'\n",
    "               ]\n",
    "videos = pd.read_csv(input_file,\n",
    "                     sep='¶',\n",
    "                     quotechar='þ',\n",
    "                     engine='python',\n",
    "                     index_col=None,\n",
    "                     skiprows=[1]\n",
    "                     )\n",
    "\n",
    "len(videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = videos.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos.to_csv(output_file,\n",
    "                header=column_names,\n",
    "                sep='¶',\n",
    "                quotechar='þ',\n",
    "                index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the comment files into one file and drop duplicates while we are at it.\n",
    "\n",
    "#output_file = 'output/right/comments_right.csv'\n",
    "#input_files = 'output/right/raw_data/comments_right_*.csv'\n",
    "\n",
    "output_file = 'output/left/comments_left.csv'\n",
    "input_files = 'output/left/raw_data/comments_left_*.csv'\n",
    "\n",
    "column_names = ['video_id',\n",
    "              'comment_id',\n",
    "              'author_display_name',\n",
    "              'author_channel_url',\n",
    "              'author_channel_id',\n",
    "              'comment_text',\n",
    "              'comment_like_count',\n",
    "              'comment_dislike_count',\n",
    "              'comment_time',\n",
    "              'reply_count']\n",
    "\n",
    "\n",
    "\n",
    "for filename in glob.glob(input_files):\n",
    "    for chunk in pd.read_csv(filename, chunksize=2000000, names=column_names):\n",
    "        df = chunk.drop_duplicates()\n",
    "        if not os.path.isfile(output_file):\n",
    "               df.to_csv(output_file, \n",
    "                         mode='a', \n",
    "                         header=column_names,\n",
    "                         sep='¶',\n",
    "                         quotechar='þ'\n",
    "                        )\n",
    "        else: \n",
    "               df.to_csv(output_file, \n",
    "                         mode='a', \n",
    "                         sep='¶',\n",
    "                         quotechar='þ',\n",
    "                         header=False \n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'output/right/recommendations_right.csv'\n",
    "input_files = 'output/right/raw_data/recommendations_right_*.csv'\n",
    "\n",
    "#output_file = 'output/left/recommendations_left.csv'\n",
    "#input_files = 'output/left/raw_data/recommendations_left_*.csv'\n",
    "\n",
    "\n",
    "column_names = ['source_video_id',\n",
    "                'target_video_id',\n",
    "                'published_at', \n",
    "                'channel_id',\n",
    "                'video_title',\n",
    "                'video_description']\n",
    "\n",
    "\n",
    "for filename in glob.glob(input_files):\n",
    "    for chunk in pd.read_csv(filename, chunksize=2000000):\n",
    "        df = chunk.drop_duplicates()\n",
    "        if not os.path.isfile(output_file):\n",
    "               df.to_csv(output_file, \n",
    "                         mode='a', \n",
    "                         header=column_names,\n",
    "                         sep='¶',\n",
    "                         quotechar='þ'\n",
    "                        )\n",
    "        else: \n",
    "               df.to_csv(output_file, \n",
    "                         mode='a', \n",
    "                         sep='¶',\n",
    "                         quotechar='þ',\n",
    "                         header=False \n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = pd.read_csv(output_file, \n",
    "                    low_memory=False,\n",
    "                    sep='¶',\n",
    "                    quotechar='þ',\n",
    "                    names=column_names\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = recs.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs.to_csv(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'output/right/transcripts_right.csv'\n",
    "input_files = 'output/right/raw_data/transcripts_right/*.vtt'\n",
    "\n",
    "#output_file = 'output/left/transcripts_left.csv'\n",
    "#input_files = 'output/left/raw_data/transcripts_left/*.vtt'\n",
    "\n",
    "\n",
    "for filename in glob.glob(input_files):\n",
    "    video_id_transcripts = ytc.transcripts.extract_transcripts(filename)\n",
    "    ytc.transcripts.write_transcripts(output_file, video_id_transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move rows from left to right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "right = pd.read_csv('output/right/channels_right.csv')\n",
    "left = pd.read_csv('output/left/channels_left.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_to_remove = [ 'John B. Wells The ONLY Official CTM Channel', \n",
    "                            'Epimetheus',\n",
    "                            'Safiya Nygaard',\n",
    "                            'Echo Gillette',\n",
    "                            'VanDeGraph',\n",
    "                            'Press For Truth',\n",
    "                            'AtheismTV',\n",
    "                            'Shädbase',\n",
    "                            'Binary Rose',\n",
    "                            'Brotcrunsher',\n",
    "                            'Arzamas',\n",
    "                            'VisualPolitik EN',\n",
    "                            'Maffick',\n",
    "                            'The Natural Farmer',\n",
    "                            'Freedom in Thought',\n",
    "                            'GabrielleBernstein',\n",
    "                            'Gregory B. Sadler',\n",
    "                            'Gray Winsler',\n",
    "                            'The Financial Diet',\n",
    "                            'Invisible People',\n",
    "                            'Jack Buckingham',\n",
    "                            'John Beckley',\n",
    "                            'Inštitut za delavske študije',\n",
    "                            'Infrarouge',\n",
    "                            'Johnny Ringo',\n",
    "                            'SWIM',\n",
    "                            'blndsundoll4mj',\n",
    "                            'Ingrid Nilsen',\n",
    "                            'Kendall Rae',\n",
    "                            'JaclynGlenn',\n",
    "                            'The Dollar Vigilante',\n",
    "                            'AMTV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_to_move = ['Drunken Peasants', \"Ken O'Keefe\", 'Kraut',\n",
    "           'Project Veritas Action', 'Aydin Paladin', 'Ben Swann',\n",
    "            'Jesse Lee Peterson', 'The Official Hagmann Report',\n",
    "            'Jonathan Pie', 'JFG Livestreams','Jack Buckby',\n",
    "            'Thunderf00t', 'SotomayorTV2', '1on1 with Tommy Sotomayor',\n",
    "            'The Liberty Hound','Hunter Avallone', 'Undoomed',\n",
    "            'Lionel Nation', 'The Still Report']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_new = pd.DataFrame()\n",
    "\n",
    "for channel in channels_to_remove:\n",
    "\n",
    "    cond = left.channel_title == channel\n",
    "    rows = left.loc[cond, :]\n",
    "    left.drop(rows.index, inplace=True)\n",
    "    left_new.append(left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chans_left = left[~left['channel_title'].isin(channels_to_remove)]\n",
    "left.to_csv('output/left/channels_left.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chans_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in channels_to_move:\n",
    "\n",
    "    cond = right.channel_title == channel\n",
    "    rows = right.loc[cond, :]\n",
    "    right = right.append(rows, ignore_index=True)\n",
    "    left.drop(rows.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right.to_csv('output/right/channels_right.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['video_id',\n",
    "              'video_published',\n",
    "              'channel_id',\n",
    "              'video_title',\n",
    "              'video_description',\n",
    "              'video_channel_title',\n",
    "              'video_tags',\n",
    "              'video_category_id',\n",
    "              'video_default_language',\n",
    "              'video_duration',\n",
    "              'video_view_count',\n",
    "              'video_comment_count',\n",
    "              'video_likes_count',\n",
    "              'video_dislikes_count',\n",
    "              'video_topic_ids',\n",
    "              'video_topic_categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vids_left = pd.read_csv('output/left/videos_left.csv', \n",
    "                        names=column_names,\n",
    "                        sep='¶',\n",
    "                        quotechar='þ',\n",
    "                        engine='python'\n",
    "                       )\n",
    "len(vids_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vids_left.video_channel_title.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vids_right = pd.read_csv('output/right/videos_right.csv',\n",
    "                        names=column_names,\n",
    "                        sep='¶',\n",
    "                        quotechar='þ',\n",
    "                        engine='python')\n",
    "len(vids_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vids_right.video_channel_title.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_to_move = []\n",
    "\n",
    "for channel in channels_to_move:\n",
    "    cond = vids_left.video_channel_title == channel\n",
    "    rows_to_move = vids_left.loc[cond, :]\n",
    "    rows = vids_left[cond]\n",
    "    #vids_right = vids_right.append(rows_to_move, ignore_index=True, sort=False)\n",
    "    for row in rows_to_move['video_id']:\n",
    "        videos_to_move.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vids_right.to_csv('output/right/videos_right.csv',\n",
    "                    header=column_names,\n",
    "                    sep='¶',\n",
    "                    quotechar='þ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtm = pd.DataFrame(videos_to_move)\n",
    "vtm.to_csv('output/left/videos_to_move.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_to_move = []\n",
    "\n",
    "vtm = pd.read_csv('output/left/videos_to_move.csv')\n",
    "for video in vtm['0']:\n",
    "    videos_to_move.append(video)\n",
    "    \n",
    "len(videos_to_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_to_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vids_left.video_channel_title.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_to_remove = []\n",
    "\n",
    "for channel in channels_to_remove:\n",
    "    cond = vids_left.video_channel_title == channel\n",
    "    rows_to_remove = vids_left.loc[cond, :]\n",
    "    for row in rows_to_remove['video_id']:\n",
    "        videos_to_remove.append(row)\n",
    "vids_left = vids_left[~vids_left['video_channel_title'].isin(channels_to_remove)]\n",
    "vids_left.to_csv('output/left/videos_left.csv',\n",
    "                header=column_names,\n",
    "                sep='¶',\n",
    "                quotechar='þ')\n",
    "vids_left.video_channel_title.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(videos_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtr = pd.DataFrame(videos_to_remove)\n",
    "vtr.to_csv('output/left/videos_to_remove.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_to_remove = []\n",
    "vtr = pd.read_csv('output/left/videos_to_remove.csv')\n",
    "for video in vtr['0']:\n",
    "    videos_to_remove.append(video)\n",
    "    \n",
    "len(videos_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now do comments, transcripts and recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_to_move = set(videos_to_move)\n",
    "videos_to_remove = set(videos_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['video_id',\n",
    "      'comment_id',\n",
    "      'author_display_name',\n",
    "      'author_channel_url',\n",
    "      'author_channel_id',\n",
    "      'comment_text',\n",
    "      'comment_like_count',\n",
    "      'comment_dislike_count',\n",
    "      'comment_time',\n",
    "      'reply_count']\n",
    "\n",
    "for chunk in pd.read_csv('output/left/comments_left.csv',\n",
    "                        chunksize=1000000, \n",
    "                        names=column_names,\n",
    "                        sep='¶',\n",
    "                        quotechar='þ',\n",
    "                        engine='python'):\n",
    "    \n",
    "    # select chunk with videos we wnt to move\n",
    "    \n",
    "    to_move = chunk[chunk['video_id'].isin(videos_to_move)]\n",
    "    \n",
    "    #write them to a new file\n",
    "    \n",
    "    if not os.path.isfile('output/right/left_comments_for_right.csv'):\n",
    "        to_move.to_csv('output/right/left_comments_for_right.csv', \n",
    "                        mode='a', \n",
    "                        header=column_names,\n",
    "                        sep='¶',\n",
    "                        quotechar='þ',\n",
    "                        index=False)\n",
    "    else: \n",
    "        to_move.to_csv('output/right/left_comments_for_right.csv', \n",
    "                        mode='a', \n",
    "                        header=False,\n",
    "                        sep='¶',\n",
    "                        quotechar='þ',\n",
    "                        index=False)\n",
    "    \n",
    "    # select chunk without videos we don't want\n",
    "    to_keep = chunk[~chunk['video_id'].isin(videos_to_remove)]\n",
    "    \n",
    "    # and write them to a new file\n",
    "    if not os.path.isfile('output/left/clean_comments_left.csv'):\n",
    "        to_keep.to_csv('output/left/clean_comments_left.csv', \n",
    "                        mode='a', \n",
    "                        header=column_names,\n",
    "                        sep='¶',\n",
    "                        quotechar='þ',\n",
    "                        index=False)\n",
    "    else: \n",
    "        to_keep.to_csv('output/left/clean_comments_left.csv', \n",
    "                        mode='a', \n",
    "                        header=False,\n",
    "                        sep='¶',\n",
    "                        quotechar='þ',\n",
    "                        index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['source_video_id',\n",
    "                'target_video_id',\n",
    "                'target_video_published',\n",
    "                'target_channel_id',\n",
    "                'target_video_title']\n",
    "\n",
    "for chunk in pd.read_csv('output/left/recommendations_left.csv',\n",
    "                        chunksize=1000000, \n",
    "                        sep='¶',\n",
    "                        quotechar='þ',\n",
    "                        engine='python',\n",
    "                        names=column_names):\n",
    "    \n",
    "    # select chunk with videos we wnt to move\n",
    "    \n",
    "    to_move = chunk.loc[chunk['source_video_id'].isin(videos_to_move), :]\n",
    "    \n",
    "    #write them to a new file\n",
    "    \n",
    "    if not os.path.isfile('output/right/left_recs_for_right.csv'):\n",
    "        to_move.to_csv('output/right/left_recs_for_right.csv', \n",
    "                        mode='a', \n",
    "                        header=column_names,\n",
    "                        sep='¶',\n",
    "                        quotechar='þ',\n",
    "                        index=False)\n",
    "    else: \n",
    "        to_move.to_csv('output/right/left_recs_for_right.csv', \n",
    "                        mode='a', \n",
    "                        header=False,\n",
    "                        sep='¶',\n",
    "                        quotechar='þ',\n",
    "                        index=False)\n",
    "    # remove files that were moved\n",
    "    \n",
    "    # select chunk without videos we don't want\n",
    "    to_keep = chunk.loc[~chunk['source_video_id'].isin(videos_to_remove), :]\n",
    "    \n",
    "    \n",
    "    # and write them to a new file\n",
    "    if not os.path.isfile('output/left/clean_recommendations_left.csv'):\n",
    "        to_keep.to_csv('output/left/clean_recommendations_left.csv', \n",
    "                        mode='a', \n",
    "                        header=column_names,\n",
    "                        sep='¶',\n",
    "                        quotechar='þ',\n",
    "                        index=False)\n",
    "        to_keep.to_csv('output/left/clean_recommendations_left.csv', \n",
    "                        mode='a', \n",
    "                        header=False,\n",
    "                        sep='¶',\n",
    "                        quotechar='þ',\n",
    "                        index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_to_remove = []\n",
    "vtr = pd.read_csv('/home/dim/Documents/projecten/extremisme/youtube/Data/infospheres/output/left/videos_to_remove.csv')\n",
    "for video in vtr['0']:\n",
    "    videos_to_remove.append(video)\n",
    "    \n",
    "videos_to_remove = set(videos_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_to_move = []\n",
    "vtr = pd.read_csv('/home/dim/Documents/projecten/extremisme/youtube/Data/infospheres/output/left/videos_to_move.csv')\n",
    "for video in vtr['0']:\n",
    "    videos_to_remove.append(video)\n",
    "    \n",
    "videos_to_move = set(videos_to_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = '/home/dim/Documents/projecten/extremisme/youtube/Data/infospheres/output/right/transcripts_right.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['video_id',\n",
    "               'video_text']\n",
    "\n",
    "for chunk in pd.read_csv('/home/dim/Documents/projecten/extremisme/youtube/Data/infospheres/output/left/transcripts_left.csv',\n",
    "                        chunksize=10000, \n",
    "                        sep='¶',\n",
    "                        quotechar='þ',\n",
    "                        engine='python',\n",
    "                        names=column_names):\n",
    "    \n",
    "    # select chunk with videos we wnt to move\n",
    "    \n",
    "    to_move = chunk.loc[chunk['video_id'].isin(videos_to_move), :]\n",
    "    \n",
    "    #write them to a new file\n",
    "    \n",
    "    \n",
    "    to_move.to_csv(right, \n",
    "                    mode='a', \n",
    "                    sep='¶',\n",
    "                    quotechar='þ',)\n",
    "        \n",
    "    # remove files that were moved\n",
    "    \n",
    "    # select chunk without videos we don't want\n",
    "    to_keep = chunk.loc[~chunk['video_id'].isin(videos_to_remove), :]\n",
    "    \n",
    "    # and write them to a new file\n",
    "    if not os.path.isfile('/home/dim/Documents/projecten/extremisme/youtube/Data/infospheres/output/left/transcripts_left_extra.csv'):\n",
    "        to_keep.to_csv('/home/dim/Documents/projecten/extremisme/youtube/Data/infospheres/output/left/transcripts_left_extra.csv', \n",
    "                        mode='a', \n",
    "                        header=column_names,\n",
    "                        sep='¶',\n",
    "                        quotechar='þ',\n",
    "                        index=False)\n",
    "    else: \n",
    "        to_keep.to_csv('/home/dim/Documents/projecten/extremisme/youtube/Data/infospheres/output/left/transcripts_left_extra.csv', \n",
    "                        mode='a', \n",
    "                        header=False,\n",
    "                        sep='¶',\n",
    "                        quotechar='þ',\n",
    "                        index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge some files:\n",
    "    \n",
    "1. comments_right - left_comments_for_right\n",
    "2. recommendations_right - left_recs_for_right\n",
    "3. transcripts_right - left_transcripts_for_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using dask\n",
    "\n",
    "column_names = ['video_id',\n",
    "      'comment_id',\n",
    "      'author_display_name',\n",
    "      'author_channel_url',\n",
    "      'author_channel_id',\n",
    "      'comment_text',\n",
    "      'comment_like_count',\n",
    "      'comment_dislike_count',\n",
    "      'comment_time',\n",
    "      'reply_count']\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Read in the csv files.\n",
    "df1 = dd.read_csv('output/right/comments_right.csv',\n",
    "                    names=column_names,\n",
    "                    error_bad_lines=False, \n",
    "                    sep='¶',\n",
    "                    quotechar='þ',\n",
    "                    engine='python')\n",
    "                  \n",
    "df2 = dd.read_csv('output/right/left_comments_for_right.csv', \n",
    "                    names=column_names,\n",
    "                    error_bad_lines=False, \n",
    "                    sep='¶',\n",
    "                    quotechar='þ',\n",
    "                    engine='python')\n",
    "\n",
    "# Merge the csv files.\n",
    "df = dd.concat([df1, df2]).drop_duplicates()\n",
    "\n",
    "# Write the output.\n",
    "df.to_csv('comments_right_final.csv', \n",
    "            mode='a',\n",
    "            header=column_names,\n",
    "            sep='¶',\n",
    "            quotechar='þ',\n",
    "            index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['video_id',\n",
    "      'comment_id',\n",
    "      'author_display_name',\n",
    "      'author_channel_url',\n",
    "      'author_channel_id',\n",
    "      'comment_text',\n",
    "      'comment_like_count',\n",
    "      'comment_dislike_count',\n",
    "      'comment_time',\n",
    "      'reply_count']\n",
    "\n",
    "\n",
    "\n",
    "for chunk in pd.read_csv('output/right/left_comments_for_right.csv',\n",
    "                        chunksize=1000000, \n",
    "                        names=column_names,\n",
    "                        sep='¶',\n",
    "                        quotechar='þ',\n",
    "                        engine='python'):\n",
    "    \n",
    "    if not os.path.isfile('output/right/comments_right.csv'):\n",
    "        chunk.to_csv('output/right/comments_right.csv', \n",
    "                        mode='a', \n",
    "                        header=column_names,\n",
    "                        sep='¶',\n",
    "                        quotechar='þ',\n",
    "                        index=False)\n",
    "    else: \n",
    "        chunk.to_csv('output/right/comments_right.csv', \n",
    "                        mode='a', \n",
    "                        header=False,\n",
    "                        sep='¶',\n",
    "                        quotechar='þ',\n",
    "                        index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "correspondent",
   "language": "python",
   "name": "correspondent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
