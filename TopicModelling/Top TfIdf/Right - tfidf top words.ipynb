{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top TfIdf words for channels\n",
    "\n",
    "Methodology: similar to https://pudding.cool/2017/09/hip-hop-words/\n",
    "\n",
    "Merge all videos for each channel for every year and see what makes that channel distinctive and if it changes over time.\n",
    "\n",
    "Method:\n",
    "1. Import cleaned captions\n",
    "2. Group them by channel and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from spacy.lang.en import English\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "captions = '/home/dim/Downloads/captions_right.csv'\n",
    "videos = '/home/dim/Documents/projecten/extremisme/youtube/data/temp/bubble/right/videos_right.csv'\n",
    "\n",
    "columns = ['video_id', 'text']\n",
    "\n",
    "df1 = pd.read_csv(captions, names=columns, low_memory=False)\n",
    "df2 = pd.read_csv(videos, low_memory=False)\n",
    "df = pd.merge(df1, df2, on='video_id', how='left')\n",
    "\n",
    "del df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['video_published'] = pd.to_datetime(df['video_published'])\n",
    "df['year'] = df['video_published'].dt.year\n",
    "\n",
    "df = df.groupby(['video_channel_title', 'year'])['text'].apply(lambda x: x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Optional: Lemmatize\n",
    "\n",
    "tokenizer = English().Defaults.create_tokenizer()\n",
    "\n",
    "df.text = df.text.apply(lambda x: ' '.join([tok.lemma_ for tok in tokenizer(x)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf values\n",
    "\n",
    "### Parameter choices\n",
    "Followed the pudding hiphop blog. Terms have to appear in at least one in 50 channels (lower than with the pudding, who use one in 10, because we have a very diverse and large set of channels with topics probably changing a lot over time). Used sublinear term frequency (not 10, but 1 + log(9)), because otherwise stop words appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(min_df=.02,sublinear_tf = True)\n",
    "res = vec.fit_transform(merged_df.text)\n",
    "vocab = {value:key for key,value in vec.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for index in merged_df.index:\n",
    "    top10words = [vocab[j] for i,j in sorted(zip(res[index].data,res[index].indices),reverse=True)[:10]]\n",
    "    if len(top10words) < 10:\n",
    "        continue\n",
    "    meta = {'year':merged_df.year[index],'channel':merged_df.channel[index],'channel_id':merged_df.channel_id[index]}\n",
    "    words = ({'word{no}'.format(no=i+1):top10words[i] for i in range(10)})\n",
    "    results.append({k: v for d in [meta, words] for k, v in d.items()})\n",
    "top10words_df = pd.DataFrame(results)\n",
    "top10words_df = top10words_df[['year','channel','channel_id']+['word'+str(no) for no in range(1,11)]]\n",
    "top10words_df.to_csv('C:/hackathon/top10tfidf_per_channel.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf top 100 words (for similarity)\n",
    "\n",
    "Parameter choices same as above, but with json output to preserve list structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for index in merged_df.index:\n",
    "    top100words = [vocab[j] for i,j in sorted(zip(res[index].data,res[index].indices),reverse=True)[:100]]\n",
    "    if len(top100words) < 100:\n",
    "        continue\n",
    "    results.append({'year':merged_df.year[index],\n",
    "            'channel':merged_df.channel[index],\n",
    "            'channel_id':merged_df.channel_id[index], \n",
    "            'words':top100words})\n",
    "top100words_df = pd.DataFrame(results)\n",
    "top100words_df = top100words_df[['year','channel','channel_id','words']]\n",
    "top100words_df.to_json('C:/hackathon/top100tfidf.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'Overlap' matrix tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "channel_id = {i:{'year':top100words_df.year[i],\n",
    "                 'channel':top100words_df.channel[i],\n",
    "                 'channel_id':top100words_df.channel_id[i]} for i in top100words_df.index}\n",
    "top100words_df.words = top100words_df.words.apply(set)\n",
    "distance_matrix = np.ones((len(channel_id),len(channel_id)))\n",
    "\n",
    "for i in range(len(channel_id)):\n",
    "    for j in range(len(channel_id)):\n",
    "        if i == j:\n",
    "            continue\n",
    "        elif i > j:\n",
    "            distance_matrix[i,j] = distance_matrix[j,i]\n",
    "        else:\n",
    "            distance_matrix[i,j] = len(top100words_df.words[i] & top100words_df.words[j])/100\n",
    "\n",
    "distance_matrix[distance_matrix < .05] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "G = nx.from_numpy_matrix(distance_matrix)\n",
    "\n",
    "for i in range(len(channel_id)):\n",
    "    G.node[i].update(channel_id[i])\n",
    "#nx.write_gexf(G,'C:/hackathon/tfidf_graph.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(G,'C:/hackathon/tfidf_graph.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('C:/hackathon/merged_right.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
